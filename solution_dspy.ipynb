{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification using DSPy and Amazon Bedrock\n",
    "\n",
    "This notebook implements a document classification system using DSPy framework with Amazon Bedrock.\n",
    "DSPy provides systematic prompt optimization and more robust classification compared to manual prompting.\n",
    "\n",
    "The task is to classify financial/business news documents into 8 categories based on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DSPy if not already installed\n",
    "!pip install -q dspy-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import DSPy\n",
    "import dspy\n",
    "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Amazon Bedrock (reusing from original implementation)\n",
    "os.environ['AWS_BEARER_TOKEN_BEDROCK'] = \"ABSKQmVkcm9ja0FQSUtleS1yenVzLWF0LTUxMzA3NzcwMDczNjpzUTRORVAvZVQ5c2xSZTBOdW1DOXRsLzZ4SUxjemtnVTNsZk03d3BGbktCVld3OXNBY0RULzY2NVdLOD0=\"\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_client = boto3.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# Initialize Bedrock Runtime client for model invocation\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    response = bedrock_client.list_foundation_models()\n",
    "    print(f\"Successfully connected to Bedrock. Found {len(response['modelSummaries'])} models.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to connect to Bedrock: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DSPy Configuration for Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockDSPyLM(dspy.LM):\n",
    "    \"\"\"Custom DSPy Language Model wrapper for Amazon Bedrock\"\"\"\n",
    "    \n",
    "    def __init__(self, bedrock_runtime, model_id=\"us.anthropic.claude-sonnet-4-20250514-v1:0\", **kwargs):\n",
    "        self.bedrock_runtime = bedrock_runtime\n",
    "        self.model_id = model_id\n",
    "        self.kwargs = {\n",
    "            'max_tokens': kwargs.get('max_tokens', 1000),\n",
    "            'temperature': kwargs.get('temperature', 0.1),\n",
    "        }\n",
    "        self.history = []\n",
    "        self.provider = \"bedrock\"\n",
    "        \n",
    "    def basic_request(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Make a request to Bedrock\"\"\"\n",
    "        try:\n",
    "            # Prepare the request based on model type\n",
    "            if \"claude\" in self.model_id:\n",
    "                # Claude model format\n",
    "                body = json.dumps({\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    \"max_tokens\": kwargs.get('max_tokens', self.kwargs['max_tokens']),\n",
    "                    \"temperature\": kwargs.get('temperature', self.kwargs['temperature']),\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\"\n",
    "                })\n",
    "            else:\n",
    "                # Generic format\n",
    "                body = json.dumps({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"max_tokens\": kwargs.get('max_tokens', self.kwargs['max_tokens']),\n",
    "                    \"temperature\": kwargs.get('temperature', self.kwargs['temperature']),\n",
    "                })\n",
    "            \n",
    "            response = self.bedrock_runtime.invoke_model(\n",
    "                body=body,\n",
    "                modelId=self.model_id,\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            \n",
    "            # Extract text based on model response format\n",
    "            if \"claude\" in self.model_id:\n",
    "                return response_body['content'][0]['text']\n",
    "            else:\n",
    "                return response_body.get('completion', response_body.get('text', str(response_body)))\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in Bedrock request: {e}\")\n",
    "            time.sleep(1)  # Brief pause before potential retry\n",
    "            raise\n",
    "    \n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        \"\"\"Call the LM with a prompt\"\"\"\n",
    "        return self.basic_request(prompt, **kwargs)\n",
    "\n",
    "# Initialize DSPy with Bedrock\n",
    "lm = BedrockDSPyLM(bedrock_runtime)\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading (Reusing from Original Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse Document dataclass and DataLoader from original implementation\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Data class to represent a document\"\"\"\n",
    "    category: int\n",
    "    text: str\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Document(category={self.category}, text_length={len(self.text)})\"\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Class to handle data loading and processing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_data(filepath: str, has_labels: bool = True) -> List[Document]:\n",
    "        \"\"\"Load documents from text file\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            # First line contains the count\n",
    "            count = int(lines[0].strip())\n",
    "            \n",
    "            # Process each document\n",
    "            for i in range(1, min(count + 1, len(lines))):\n",
    "                line = lines[i].strip()\n",
    "                if has_labels:\n",
    "                    # Extract category and text\n",
    "                    parts = line.split(' ', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        category = int(parts[0])\n",
    "                        text = parts[1]\n",
    "                        documents.append(Document(category=category, text=text))\n",
    "                else:\n",
    "                    # For test data, category is in the text (we'll extract it for evaluation)\n",
    "                    parts = line.split(' ', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        category = int(parts[0])\n",
    "                        text = parts[1]\n",
    "                        documents.append(Document(category=category, text=text))\n",
    "                    \n",
    "            logger.info(f\"Loaded {len(documents)} documents from {filepath}\")\n",
    "            return documents\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data from {filepath}: {e}\")\n",
    "            raise\n",
    "\n",
    "# Load training and testing data\n",
    "train_documents = DataLoader.load_data('trainingdata.txt', has_labels=True)\n",
    "test_documents = DataLoader.load_data('testingdata.txt', has_labels=True)\n",
    "\n",
    "print(f\"Training documents: {len(train_documents)}\")\n",
    "print(f\"Testing documents: {len(test_documents)}\")\n",
    "print(f\"\\nTraining category distribution:\")\n",
    "train_df = pd.DataFrame([(d.category, len(d.text)) for d in train_documents], \n",
    "                        columns=['category', 'text_length'])\n",
    "print(train_df['category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DSPy Signatures and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DSPy Signatures for our task\n",
    "class GenerateCategoryDescription(dspy.Signature):\n",
    "    \"\"\"Generate a description for a document category based on examples.\"\"\"\n",
    "    \n",
    "    examples = dspy.InputField(desc=\"Examples of documents in this category\")\n",
    "    category_number = dspy.InputField(desc=\"Category number (1-8)\")\n",
    "    description = dspy.OutputField(desc=\"Brief description of what documents in this category contain\")\n",
    "\n",
    "class ClassifyDocument(dspy.Signature):\n",
    "    \"\"\"Classify a financial/business document into one of 8 categories.\"\"\"\n",
    "    \n",
    "    document = dspy.InputField(desc=\"The document text to classify\")\n",
    "    category_descriptions = dspy.InputField(desc=\"Descriptions of each category (1-8)\")\n",
    "    category = dspy.OutputField(desc=\"Category number (1-8)\")\n",
    "    confidence = dspy.OutputField(desc=\"Confidence score (0.0-1.0)\")\n",
    "    reasoning = dspy.OutputField(desc=\"Brief explanation of the classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DSPy Modules\n",
    "class CategoryDescriptionGenerator(dspy.Module):\n",
    "    \"\"\"Module to generate category descriptions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate = dspy.ChainOfThought(GenerateCategoryDescription)\n",
    "    \n",
    "    def forward(self, examples, category_number):\n",
    "        # Truncate examples to avoid token limits\n",
    "        truncated_examples = [ex[:200] + \"...\" for ex in examples[:3]]\n",
    "        examples_text = \"\\n\".join(truncated_examples)\n",
    "        \n",
    "        prediction = self.generate(\n",
    "            examples=examples_text,\n",
    "            category_number=str(category_number)\n",
    "        )\n",
    "        return prediction.description\n",
    "\n",
    "class DocumentClassifierDSPy(dspy.Module):\n",
    "    \"\"\"DSPy-based document classifier with Chain of Thought reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self, category_descriptions=None):\n",
    "        super().__init__()\n",
    "        self.classify = dspy.ChainOfThought(ClassifyDocument)\n",
    "        self.category_descriptions = category_descriptions or self._get_default_descriptions()\n",
    "    \n",
    "    def forward(self, document):\n",
    "        # Format category descriptions\n",
    "        desc_text = \"\\n\".join([f\"Category {k}: {v}\" for k, v in self.category_descriptions.items()])\n",
    "        \n",
    "        prediction = self.classify(\n",
    "            document=document[:500],  # Truncate to avoid token limits\n",
    "            category_descriptions=desc_text\n",
    "        )\n",
    "        \n",
    "        # Parse and validate the output\n",
    "        try:\n",
    "            category = int(prediction.category)\n",
    "            category = max(1, min(8, category))  # Ensure valid range\n",
    "        except:\n",
    "            category = 1  # Default\n",
    "        \n",
    "        try:\n",
    "            confidence = float(prediction.confidence)\n",
    "            confidence = max(0.0, min(1.0, confidence))  # Ensure valid range\n",
    "        except:\n",
    "            confidence = 0.5  # Default\n",
    "        \n",
    "        return dspy.Prediction(\n",
    "            category=category,\n",
    "            confidence=confidence,\n",
    "            reasoning=prediction.reasoning\n",
    "        )\n",
    "    \n",
    "    def _get_default_descriptions(self):\n",
    "        \"\"\"Default category descriptions\"\"\"\n",
    "        return {\n",
    "            1: \"Corporate earnings reports, financial results, stock splits, company performance\",\n",
    "            2: \"Mergers, acquisitions, corporate takeovers, business deals, company purchases\",\n",
    "            3: \"International trade, trade policies, GATT, EC regulations, import/export policies\",\n",
    "            4: \"Shipping, maritime incidents, port operations, transportation, ferry disasters\",\n",
    "            5: \"Agriculture, grain markets, farming, crop reports, food production\",\n",
    "            6: \"Oil/energy industry, petroleum markets, OPEC, crude oil prices, refineries\",\n",
    "            7: \"Banking, interest rates, financial policy, central banks, monetary policy\",\n",
    "            8: \"Currency markets, foreign exchange, international finance, dollar movements\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Category Descriptions using DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categories (reusing from original)\n",
    "def analyze_categories(documents: List[Document]) -> Dict[int, List[str]]:\n",
    "    \"\"\"Group documents by category for analysis\"\"\"\n",
    "    categories = {}\n",
    "    for doc in documents:\n",
    "        if doc.category not in categories:\n",
    "            categories[doc.category] = []\n",
    "        categories[doc.category].append(doc.text)\n",
    "    return categories\n",
    "\n",
    "training_categories = analyze_categories(train_documents)\n",
    "\n",
    "# Generate descriptions using DSPy\n",
    "print(\"Generating category descriptions using DSPy...\\n\")\n",
    "desc_generator = CategoryDescriptionGenerator()\n",
    "category_descriptions = {}\n",
    "\n",
    "for cat in sorted(training_categories.keys()):\n",
    "    try:\n",
    "        description = desc_generator(training_categories[cat], cat)\n",
    "        category_descriptions[cat] = description\n",
    "        print(f\"Category {cat}: {description}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to generate description for category {cat}: {e}\")\n",
    "        # Use default description\n",
    "        default_desc = DocumentClassifierDSPy()._get_default_descriptions().get(cat, \"\")\n",
    "        category_descriptions[cat] = default_desc\n",
    "        print(f\"Category {cat}: {default_desc} (default)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Training Examples for DSPy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert documents to DSPy examples\n",
    "def create_dspy_examples(documents: List[Document], limit: int = None) -> List[dspy.Example]:\n",
    "    \"\"\"Convert documents to DSPy examples for training\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    docs_to_process = documents[:limit] if limit else documents\n",
    "    \n",
    "    for doc in docs_to_process:\n",
    "        example = dspy.Example(\n",
    "            document=doc.text[:500],  # Truncate for efficiency\n",
    "            category=str(doc.category)\n",
    "        ).with_inputs('document')\n",
    "        examples.append(example)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Create training and validation sets\n",
    "# Use a subset for optimization to manage API costs and time\n",
    "train_examples = create_dspy_examples(train_documents, limit=50)\n",
    "val_examples = create_dspy_examples(train_documents[50:70])  # Small validation set\n",
    "\n",
    "print(f\"Created {len(train_examples)} training examples\")\n",
    "print(f\"Created {len(val_examples)} validation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DSPy Optimization with Bootstrap Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric for optimization\n",
    "def classification_metric(example, pred, trace=None):\n",
    "    \"\"\"Metric for evaluating classification accuracy\"\"\"\n",
    "    try:\n",
    "        # Extract predicted category\n",
    "        pred_category = str(pred.category) if hasattr(pred, 'category') else str(pred)\n",
    "        true_category = str(example.category)\n",
    "        \n",
    "        # Check if prediction matches\n",
    "        return pred_category == true_category\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier_dspy = DocumentClassifierDSPy(category_descriptions=category_descriptions)\n",
    "\n",
    "print(\"Starting DSPy optimization...\")\n",
    "print(\"This will automatically optimize prompts based on training examples.\\n\")\n",
    "\n",
    "# Configure the optimizer\n",
    "optimizer = BootstrapFewShot(\n",
    "    metric=classification_metric,\n",
    "    max_bootstrapped_demos=3,  # Number of demonstrations to include\n",
    "    max_labeled_demos=5,  # Maximum labeled examples to use\n",
    "    max_rounds=2,  # Number of optimization rounds\n",
    ")\n",
    "\n",
    "# Optimize the classifier\n",
    "try:\n",
    "    optimized_classifier = optimizer.compile(\n",
    "        classifier_dspy,\n",
    "        trainset=train_examples,\n",
    "        valset=val_examples\n",
    "    )\n",
    "    print(\"\\n✓ DSPy optimization completed successfully!\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Optimization failed: {e}. Using unoptimized classifier.\")\n",
    "    optimized_classifier = classifier_dspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Classify Test Documents with Optimized DSPy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify documents with DSPy\n",
    "def classify_with_dspy(classifier, documents: List[Document]) -> List[Tuple[int, float, str]]:\n",
    "    \"\"\"Classify documents using DSPy classifier\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for doc in tqdm(documents, desc=\"Classifying with DSPy\"):\n",
    "        try:\n",
    "            # Add small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            # Classify document\n",
    "            prediction = classifier(document=doc.text)\n",
    "            \n",
    "            category = prediction.category\n",
    "            confidence = prediction.confidence\n",
    "            reasoning = prediction.reasoning if hasattr(prediction, 'reasoning') else \"\"\n",
    "            \n",
    "            results.append((category, confidence, reasoning))\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error: {e}\")\n",
    "            results.append((1, 0.5, \"Error in classification\"))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Classify test documents\n",
    "print(f\"Classifying {len(test_documents)} test documents with optimized DSPy classifier...\")\n",
    "print(\"This may take a few minutes due to API rate limits.\\n\")\n",
    "\n",
    "dspy_predictions = classify_with_dspy(optimized_classifier, test_documents)\n",
    "\n",
    "# Extract results\n",
    "y_true = [doc.category for doc in test_documents]\n",
    "y_pred_dspy = [pred[0] for pred in dspy_predictions]\n",
    "confidences_dspy = [pred[1] for pred in dspy_predictions]\n",
    "reasonings_dspy = [pred[2] for pred in dspy_predictions]\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df_dspy = pd.DataFrame({\n",
    "    'true_category': y_true,\n",
    "    'predicted_category': y_pred_dspy,\n",
    "    'confidence': confidences_dspy,\n",
    "    'reasoning': reasonings_dspy,\n",
    "    'correct': [t == p for t, p in zip(y_true, y_pred_dspy)],\n",
    "    'text_preview': [doc.text[:100] + '...' for doc in test_documents]\n",
    "})\n",
    "\n",
    "print(\"\\nDSPy Classification Results Summary:\")\n",
    "print(f\"Total documents: {len(results_df_dspy)}\")\n",
    "print(f\"Correct predictions: {results_df_dspy['correct'].sum()}\")\n",
    "print(f\"Accuracy: {results_df_dspy['correct'].mean():.2%}\")\n",
    "print(f\"Average confidence: {results_df_dspy['confidence'].mean():.2f}\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample DSPy Predictions:\")\n",
    "print(results_df_dspy[['true_category', 'predicted_category', 'confidence', 'correct']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Metrics (Reusing from Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate detailed metrics\n",
    "print(\"\\nDetailed DSPy Classification Report:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_true, y_pred_dspy, target_names=[f\"Category {i}\" for i in range(1, 9)]))\n",
    "\n",
    "# Calculate F1 scores\n",
    "f1_macro_dspy = f1_score(y_true, y_pred_dspy, average='macro')\n",
    "f1_weighted_dspy = f1_score(y_true, y_pred_dspy, average='weighted')\n",
    "f1_per_class_dspy = f1_score(y_true, y_pred_dspy, average=None)\n",
    "\n",
    "print(\"\\nDSPy F1 Scores:\")\n",
    "print(f\"Macro F1 Score: {f1_macro_dspy:.3f}\")\n",
    "print(f\"Weighted F1 Score: {f1_weighted_dspy:.3f}\")\n",
    "print(\"\\nPer-class F1 Scores:\")\n",
    "for i, score in enumerate(f1_per_class_dspy, 1):\n",
    "    if not np.isnan(score):  # Only print if class exists in test set\n",
    "        print(f\"  Category {i}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm_dspy = confusion_matrix(y_true, y_pred_dspy)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_dspy, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(1, 9), yticklabels=range(1, 9))\n",
    "plt.title('DSPy Confusion Matrix for Document Classification', fontsize=16)\n",
    "plt.xlabel('Predicted Category', fontsize=12)\n",
    "plt.ylabel('True Category', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Analysis:\")\n",
    "print(\"- Diagonal values show correct predictions\")\n",
    "print(\"- Off-diagonal values show misclassifications\")\n",
    "print(\"- Higher values on diagonal indicate better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Analysis and DSPy Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassifications\n",
    "misclassified_dspy = results_df_dspy[~results_df_dspy['correct']]\n",
    "\n",
    "print(f\"\\nDSPy Misclassification Analysis:\")\n",
    "print(f\"Total misclassified: {len(misclassified_dspy)} out of {len(results_df_dspy)} ({len(misclassified_dspy)/len(results_df_dspy)*100:.1f}%)\")\n",
    "\n",
    "if len(misclassified_dspy) > 0:\n",
    "    print(\"\\nMisclassification patterns:\")\n",
    "    confusion_pairs = misclassified_dspy.groupby(['true_category', 'predicted_category']).size().reset_index(name='count')\n",
    "    confusion_pairs = confusion_pairs.sort_values('count', ascending=False)\n",
    "    \n",
    "    for _, row in confusion_pairs.head(5).iterrows():\n",
    "        print(f\"  True: Category {row['true_category']} -> Predicted: Category {row['predicted_category']}: {row['count']} times\")\n",
    "    \n",
    "    print(\"\\nLow confidence predictions:\")\n",
    "    low_conf = results_df_dspy[results_df_dspy['confidence'] < 0.7].sort_values('confidence')\n",
    "    if len(low_conf) > 0:\n",
    "        print(f\"Found {len(low_conf)} predictions with confidence < 0.7\")\n",
    "        for _, row in low_conf.head(3).iterrows():\n",
    "            print(f\"  Doc preview: {row['text_preview'][:50]}...\")\n",
    "            print(f\"    True: {row['true_category']}, Predicted: {row['predicted_category']}, Confidence: {row['confidence']:.2f}\")\n",
    "            if row['reasoning']:\n",
    "                print(f\"    Reasoning: {row['reasoning'][:100]}...\")\n",
    "else:\n",
    "    print(\"Perfect classification! No misclassifications found.\")\n",
    "\n",
    "# Analyze DSPy reasoning patterns\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DSPy Reasoning Analysis:\")\n",
    "print(\"Sample reasoning for correct predictions:\")\n",
    "correct_with_reasoning = results_df_dspy[results_df_dspy['correct'] & (results_df_dspy['reasoning'] != '')]\n",
    "for _, row in correct_with_reasoning.head(3).iterrows():\n",
    "    print(f\"\\nCategory {row['true_category']} (Confidence: {row['confidence']:.2f}):\")\n",
    "    print(f\"  {row['reasoning'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Comparison with Manual Prompting (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've run the original notebook, you can load and compare results\n",
    "try:\n",
    "    # Try to load results from the original implementation\n",
    "    original_results = pd.read_csv('classification_results.csv')\n",
    "    \n",
    "    print(\"Comparison: DSPy vs Manual Prompting\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate metrics for comparison\n",
    "    original_accuracy = original_results['correct'].mean()\n",
    "    dspy_accuracy = results_df_dspy['correct'].mean()\n",
    "    \n",
    "    print(f\"Manual Prompting Accuracy: {original_accuracy:.2%}\")\n",
    "    print(f\"DSPy Accuracy: {dspy_accuracy:.2%}\")\n",
    "    print(f\"Improvement: {(dspy_accuracy - original_accuracy)*100:+.1f}%\")\n",
    "    \n",
    "    print(f\"\\nManual Prompting Avg Confidence: {original_results['confidence'].mean():.2f}\")\n",
    "    print(f\"DSPy Avg Confidence: {results_df_dspy['confidence'].mean():.2f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Original results not found. Run the original notebook first to compare.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load original results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. DSPy Advantages and Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Report\n",
    "print(\"=\"*80)\n",
    "print(\"DSPY CLASSIFICATION SYSTEM REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DSPY ADVANTAGES DEMONSTRATED:\")\n",
    "print(\"   ✓ Systematic prompt optimization instead of manual engineering\")\n",
    "print(\"   ✓ Automatic few-shot learning with best examples selection\")\n",
    "print(\"   ✓ Built-in Chain-of-Thought reasoning\")\n",
    "print(\"   ✓ Cleaner, more maintainable code structure\")\n",
    "print(\"   ✓ Reproducible optimization process\")\n",
    "\n",
    "print(\"\\n2. MODEL CONFIGURATION:\")\n",
    "print(f\"   - Framework: DSPy with Amazon Bedrock\")\n",
    "print(f\"   - Model: Claude Sonnet via Bedrock\")\n",
    "print(f\"   - Optimization: BootstrapFewShot with {len(train_examples)} training examples\")\n",
    "print(f\"   - Strategy: Automatic prompt optimization with Chain-of-Thought\")\n",
    "\n",
    "print(\"\\n3. PERFORMANCE METRICS:\")\n",
    "print(f\"   - Accuracy: {results_df_dspy['correct'].mean():.2%}\")\n",
    "print(f\"   - F1 Score (Macro): {f1_macro_dspy:.3f}\")\n",
    "print(f\"   - F1 Score (Weighted): {f1_weighted_dspy:.3f}\")\n",
    "print(f\"   - Average Confidence: {results_df_dspy['confidence'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n4. ROBUSTNESS IMPROVEMENTS:\")\n",
    "print(\"   - Less sensitive to prompt wording variations\")\n",
    "print(\"   - Automatic selection of optimal examples\")\n",
    "print(\"   - Systematic optimization based on validation metrics\")\n",
    "print(\"   - Better generalization through programmatic prompting\")\n",
    "\n",
    "print(\"\\n5. CATEGORY DESCRIPTIONS (DSPy Generated):\")\n",
    "for cat_id, desc in sorted(category_descriptions.items()):\n",
    "    print(f\"   Category {cat_id}: {desc[:60]}...\")\n",
    "\n",
    "print(\"\\n6. FUTURE IMPROVEMENTS:\")\n",
    "print(\"   - Experiment with different DSPy optimizers (e.g., MIPRO)\")\n",
    "print(\"   - Increase training examples for better optimization\")\n",
    "print(\"   - Use DSPy's ensemble capabilities for higher accuracy\")\n",
    "print(\"   - Implement DSPy's retrieval augmentation for better context\")\n",
    "print(\"   - Fine-tune temperature and other hyperparameters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DSPy implementation successfully completed!\")\n",
    "print(\"The system is now more robust and systematically optimized.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save DSPy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DSPy classification results\n",
    "output_filename_dspy = 'classification_results_dspy.csv'\n",
    "results_df_dspy.to_csv(output_filename_dspy, index=False)\n",
    "print(f\"DSPy results saved to {output_filename_dspy}\")\n",
    "\n",
    "# Save DSPy category descriptions\n",
    "with open('category_descriptions_dspy.json', 'w') as f:\n",
    "    json.dump(category_descriptions, f, indent=2)\n",
    "print(\"DSPy category descriptions saved to category_descriptions_dspy.json\")\n",
    "\n",
    "# Save the optimized classifier configuration\n",
    "try:\n",
    "    # Save DSPy traces for analysis\n",
    "    with open('dspy_optimization_info.txt', 'w') as f:\n",
    "        f.write(\"DSPy Optimization Summary\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"Training examples used: {len(train_examples)}\\n\")\n",
    "        f.write(f\"Validation examples used: {len(val_examples)}\\n\")\n",
    "        f.write(f\"Final accuracy: {results_df_dspy['correct'].mean():.2%}\\n\")\n",
    "        f.write(f\"F1 Score (Macro): {f1_macro_dspy:.3f}\\n\")\n",
    "        f.write(\"\\nOptimized prompts and demonstrations are embedded in the classifier.\\n\")\n",
    "    print(\"DSPy optimization info saved to dspy_optimization_info.txt\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save optimization info: {e}\")\n",
    "\n",
    "print(\"\\n✓ All DSPy results and configurations saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
